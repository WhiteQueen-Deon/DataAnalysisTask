{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Part 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import_libs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 618, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1951, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17532\\2360153633.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py\", line 38, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 29, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 618, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1951, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17532\\2360153633.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions",
   "metadata": {},
   "source": [
    "## Part 2: Utility Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clean_price",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì clean_price function defined\n"
     ]
    }
   ],
   "source": [
    "def clean_price(price_str):\n",
    "    \"\"\"\n",
    "    Clean and standardize price strings to USD floats.\n",
    "    Handles formats: '$27.00', '‚Ç¨50¬¢50', 'USD 45.99', 'EUR 71.00'\n",
    "    Converts EUR to USD: ‚Ç¨1 = $1.2\n",
    "    \"\"\"\n",
    "    if pd.isna(price_str):\n",
    "        return None\n",
    "\n",
    "    price_str = str(price_str).strip()\n",
    "    is_euro = ('‚Ç¨' in price_str) or ('EUR' in price_str.upper())\n",
    "\n",
    "    # Remove currency symbols\n",
    "    price_str = price_str.replace('$', '').replace('‚Ç¨', '').replace('USD', '')\n",
    "    price_str = price_str.replace('EUR', '').replace('¬¢', '.').strip()\n",
    "    \n",
    "    try:\n",
    "        price = float(price_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # Convert EUR to USD\n",
    "    if is_euro:\n",
    "        price = price * 1.2\n",
    "    \n",
    "    return price\n",
    "\n",
    "print(\"‚úì clean_price function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clean_timestamp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì clean_timestamp function defined\n"
     ]
    }
   ],
   "source": [
    "def clean_timestamp(timestamp_str):\n",
    "    \"\"\"\n",
    "    Parse various timestamp formats into pandas datetime.\n",
    "    Handles ISO 8601 (YYYY-MM-DD) and European formats (DD.MM.YYYY)\n",
    "    \"\"\"\n",
    "    if pd.isna(timestamp_str):\n",
    "        return None\n",
    "    \n",
    "    timestamp_str = str(timestamp_str).strip()\n",
    "    timestamp_str = timestamp_str.replace(';', ' ').replace(',', ' ')\n",
    "    timestamp_str = timestamp_str.replace('A.M.', 'AM').replace('P.M.', 'PM')\n",
    "    \n",
    "    # Check if ISO format (YYYY-MM-DD)\n",
    "    iso_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "    if re.search(iso_pattern, timestamp_str):\n",
    "        result = pd.to_datetime(timestamp_str, errors='coerce', dayfirst=False)\n",
    "    else:\n",
    "        result = pd.to_datetime(timestamp_str, errors='coerce', dayfirst=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úì clean_timestamp function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unionfind",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì UnionFind class defined\n"
     ]
    }
   ],
   "source": [
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union-Find (Disjoint Set Union) data structure with path compression.\n",
    "    Used for efficiently grouping duplicate users.\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    \n",
    "    def find(self, x):\n",
    "        \"\"\"Find root with path compression\"\"\"\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        \"\"\"Merge two sets\"\"\"\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        if root_x != root_y:\n",
    "            self.parent[root_x] = root_y\n",
    "    \n",
    "    def count_groups(self):\n",
    "        \"\"\"Count number of disjoint sets\"\"\"\n",
    "        return len(set(self.find(i) for i in range(len(self.parent))))\n",
    "    \n",
    "    def get_group_members(self, x):\n",
    "        \"\"\"Get all members in the same group as x\"\"\"\n",
    "        root = self.find(x)\n",
    "        return [i for i in range(len(self.parent)) if self.find(i) == root]\n",
    "\n",
    "print(\"‚úì UnionFind class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_analysis",
   "metadata": {},
   "source": [
    "## Part 3: Main Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "analyze_dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì analyze_dataset function defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Perform complete analysis on a dataset.\n",
    "    Returns dictionary with all 6 task results.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä ANALYZING: {dataset_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # ===== 1. LOAD DATA =====\n",
    "    print(\"üìÇ Loading data...\")\n",
    "    orders_df = pd.read_parquet(f'{dataset_name}/orders.parquet', engine='fastparquet')\n",
    "    users_df = pd.read_csv(f'{dataset_name}/users.csv')\n",
    "    \n",
    "    with open(f'{dataset_name}/books.yaml', 'r', encoding='utf-8') as file:\n",
    "        books_data = yaml.safe_load(file)\n",
    "    books_df = pd.DataFrame(books_data)\n",
    "    books_df.columns = books_df.columns.str.replace(':', '')\n",
    "    \n",
    "    print(f\"   ‚úì Loaded {len(orders_df):,} orders\")\n",
    "    print(f\"   ‚úì Loaded {len(users_df):,} users\")\n",
    "    print(f\"   ‚úì Loaded {len(books_df):,} books\\n\")\n",
    "    \n",
    "    # ===== 2. CLEAN ORDERS =====\n",
    "    print(\"üßπ Cleaning orders data...\")\n",
    "    orders_df['clean_price'] = orders_df['unit_price'].apply(clean_price)\n",
    "    orders_df['clean_timestamp'] = orders_df['timestamp'].apply(clean_timestamp)\n",
    "    \n",
    "    before_clean = len(orders_df)\n",
    "    orders_df = orders_df.dropna(subset=['clean_price', 'clean_timestamp'])\n",
    "    after_clean = len(orders_df)\n",
    "    \n",
    "    orders_df['paid_price'] = orders_df['quantity'] * orders_df['clean_price']\n",
    "    orders_df['date'] = orders_df['clean_timestamp'].dt.date\n",
    "    orders_df = orders_df.drop_duplicates(subset=['id'])\n",
    "    \n",
    "    print(f\"   ‚úì Removed {before_clean - after_clean:,} invalid rows\")\n",
    "    print(f\"   ‚úì Final orders count: {len(orders_df):,}\\n\")\n",
    "    \n",
    "    # ===== 3. BUILD UNION-FIND FOR USERS =====\n",
    "    print(\"üîó Building user deduplication structure...\")\n",
    "    n_users = len(users_df)\n",
    "    uf = UnionFind(n_users)\n",
    "    users_clean = users_df.copy()\n",
    "    \n",
    "    # Fill missing addresses\n",
    "    na_mask = users_clean['address'].isna()\n",
    "    na_count = na_mask.sum()\n",
    "    users_clean.loc[na_mask, 'address'] = [f'MISSING_ADDRESS_{i}' for i in range(na_count)]\n",
    "    \n",
    "    # Union users with matching fields\n",
    "    for field in ['email', 'phone', 'name', 'address']:\n",
    "        groups = users_clean.groupby(field).groups\n",
    "        for indices in groups.values():\n",
    "            if len(indices) > 1:\n",
    "                first = indices[0]\n",
    "                for idx in indices[1:]:\n",
    "                    uf.union(first, idx)\n",
    "    \n",
    "    print(f\"   ‚úì User deduplication complete\\n\")\n",
    "    \n",
    "    # ===== EXECUTE 6 TASKS =====\n",
    "    results = {}\n",
    "    \n",
    "    # --- TASK 1: Top 5 Revenue Days ---\n",
    "    print(\"üìÖ Task 1: Top 5 Revenue Days\")\n",
    "    top5_days = orders_df.groupby('date')['paid_price'].sum().sort_values(ascending=False).head(5)\n",
    "    results['task1'] = top5_days\n",
    "    for i, (date, revenue) in enumerate(top5_days.items(), 1):\n",
    "        print(f\"   {i}. {date}: ${revenue:,.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # --- TASK 2: Unique Users ---\n",
    "    print(\"üë• Task 2: Real Unique Users\")\n",
    "    unique_users = uf.count_groups()\n",
    "    results['task2'] = unique_users\n",
    "    print(f\"   Total records: {len(users_df):,}\")\n",
    "    print(f\"   Real unique users: {unique_users:,}\")\n",
    "    print(f\"   Duplicates found: {len(users_df) - unique_users:,}\\n\")\n",
    "    \n",
    "    # --- TASK 3: Author Sets ---\n",
    "    print(\"‚úçÔ∏è  Task 3: Unique Author Sets\")\n",
    "    author_sets = set()\n",
    "    for author_str in books_df['author']:\n",
    "        if pd.isna(author_str):\n",
    "            continue\n",
    "        authors = [a.strip() for a in str(author_str).split(',')]\n",
    "        author_tuple = tuple(sorted(authors))\n",
    "        author_sets.add(author_tuple)\n",
    "    results['task3'] = len(author_sets)\n",
    "    print(f\"   Unique author sets: {len(author_sets):,}\\n\")\n",
    "    \n",
    "    # --- TASK 4: Most Popular Author ---\n",
    "    print(\"‚≠ê Task 4: Most Popular Author\")\n",
    "    book_sales = orders_df.groupby('book_id')['quantity'].sum()\n",
    "    books_with_sales = books_df.merge(book_sales, left_on='id', right_index=True, how='left')\n",
    "    books_with_sales['quantity'] = books_with_sales['quantity'].fillna(0)\n",
    "    \n",
    "    def standardize_author(author_str):\n",
    "        if pd.isna(author_str):\n",
    "            return ()\n",
    "        authors = [a.strip() for a in str(author_str).split(',')]\n",
    "        return tuple(sorted(authors))\n",
    "    \n",
    "    books_with_sales['author_set'] = books_with_sales['author'].apply(standardize_author)\n",
    "    author_sales = books_with_sales.groupby('author_set')['quantity'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    most_popular = author_sales.idxmax()\n",
    "    max_sales = author_sales.max()\n",
    "    results['task4'] = {'authors': most_popular, 'books_sold': int(max_sales)}\n",
    "    \n",
    "    author_name = most_popular[0] if len(most_popular) == 1 else ' & '.join(most_popular)\n",
    "    print(f\"   Most popular: {author_name}\")\n",
    "    print(f\"   Books sold: {int(max_sales):,}\\n\")\n",
    "    \n",
    "    # --- TASK 5: Top Customer ---\n",
    "    print(\"üí∞ Task 5: Top Customer\")\n",
    "    user_spending = orders_df.groupby('user_id')['paid_price'].sum()\n",
    "    real_user_spending = {}\n",
    "    \n",
    "    for user_id, spending in user_spending.items():\n",
    "        user_rows = users_clean[users_clean['id'] == user_id]\n",
    "        if len(user_rows) == 0:\n",
    "            continue\n",
    "        user_idx = user_rows.index[0]\n",
    "        root = uf.find(user_idx)\n",
    "        if root not in real_user_spending:\n",
    "            real_user_spending[root] = 0\n",
    "        real_user_spending[root] += spending\n",
    "    \n",
    "    top_real_user = max(real_user_spending, key=real_user_spending.get)\n",
    "    max_spending = real_user_spending[top_real_user]\n",
    "    all_user_indices = uf.get_group_members(top_real_user)\n",
    "    all_user_ids = users_clean.loc[all_user_indices, 'id'].tolist()\n",
    "    \n",
    "    results['task5'] = {'user_ids': all_user_ids, 'total_spending': max_spending}\n",
    "    print(f\"   Total spending: ${max_spending:,.2f}\")\n",
    "    print(f\"   User IDs: {all_user_ids}\")\n",
    "    print(f\"   Number of aliases: {len(all_user_ids)}\\n\")\n",
    "    \n",
    "    # --- TASK 6: Daily Revenue Chart ---\n",
    "    print(\"üìà Task 6: Daily Revenue Chart\")\n",
    "    daily_revenue = orders_df.groupby('date')['paid_price'].sum().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(daily_revenue.index, daily_revenue.values, \n",
    "             marker='o', linestyle='-', linewidth=2, markersize=4, color='#2E86AB')\n",
    "    plt.fill_between(daily_revenue.index, daily_revenue.values, alpha=0.2, color='#2E86AB')\n",
    "    \n",
    "    plt.title(f'Daily Revenue Over Time - {dataset_name}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Revenue ($)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    chart_filename = f'daily_revenue_{dataset_name}.png'\n",
    "    plt.savefig(chart_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    results['task6'] = chart_filename\n",
    "    print(f\"   ‚úì Chart saved: {chart_filename}\\n\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"‚úÖ {dataset_name} ANALYSIS COMPLETE\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì analyze_dataset function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution",
   "metadata": {},
   "source": [
    "## Part 4: Execute Analysis on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "run_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä ANALYZING: DATA1\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "   ‚úì Loaded 11,237 orders\n",
      "   ‚úì Loaded 3,293 users\n",
      "   ‚úì Loaded 753 books\n",
      "\n",
      "üßπ Cleaning orders data...\n",
      "   ‚úì Removed 0 invalid rows\n",
      "   ‚úì Final orders count: 11,237\n",
      "\n",
      "üîó Building user deduplication structure...\n",
      "   ‚úì User deduplication complete\n",
      "\n",
      "üìÖ Task 1: Top 5 Revenue Days\n",
      "   1. 2024-12-17: $57,011.46\n",
      "   2. 2024-11-03: $46,258.65\n",
      "   3. 2025-03-23: $39,120.97\n",
      "   4. 2024-09-06: $32,795.31\n",
      "   5. 2025-01-25: $31,732.46\n",
      "\n",
      "üë• Task 2: Real Unique Users\n",
      "   Total records: 3,293\n",
      "   Real unique users: 3,066\n",
      "   Duplicates found: 227\n",
      "\n",
      "‚úçÔ∏è  Task 3: Unique Author Sets\n",
      "   Unique author sets: 325\n",
      "\n",
      "‚≠ê Task 4: Most Popular Author\n",
      "   Most popular: Arlinda Huel\n",
      "   Books sold: 201\n",
      "\n",
      "üí∞ Task 5: Top Customer\n",
      "   Total spending: $37,609.70\n",
      "   User IDs: [45800]\n",
      "   Number of aliases: 1\n",
      "\n",
      "üìà Task 6: Daily Revenue Chart\n",
      "   ‚úì Chart saved: daily_revenue_DATA1.png\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DATA1 ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìä ANALYZING: DATA2\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "   ‚úì Loaded 9,850 orders\n",
      "   ‚úì Loaded 2,810 users\n",
      "   ‚úì Loaded 741 books\n",
      "\n",
      "üßπ Cleaning orders data...\n",
      "   ‚úì Removed 0 invalid rows\n",
      "   ‚úì Final orders count: 9,850\n",
      "\n",
      "üîó Building user deduplication structure...\n",
      "   ‚úì User deduplication complete\n",
      "\n",
      "üìÖ Task 1: Top 5 Revenue Days\n",
      "   1. 2024-12-24: $42,137.01\n",
      "   2. 2024-08-29: $40,556.08\n",
      "   3. 2024-12-29: $39,297.21\n",
      "   4. 2025-01-30: $39,021.69\n",
      "   5. 2024-11-29: $35,207.05\n",
      "\n",
      "üë• Task 2: Real Unique Users\n",
      "   Total records: 2,810\n",
      "   Real unique users: 2,633\n",
      "   Duplicates found: 177\n",
      "\n",
      "‚úçÔ∏è  Task 3: Unique Author Sets\n",
      "   Unique author sets: 293\n",
      "\n",
      "‚≠ê Task 4: Most Popular Author\n",
      "   Most popular: Hershel Treutel & Miss Modesto Denesik & Sen. Trula Bosco\n",
      "   Books sold: 163\n",
      "\n",
      "üí∞ Task 5: Top Customer\n",
      "   Total spending: $37,051.25\n",
      "   User IDs: [53256]\n",
      "   Number of aliases: 1\n",
      "\n",
      "üìà Task 6: Daily Revenue Chart\n",
      "   ‚úì Chart saved: daily_revenue_DATA2.png\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DATA2 ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìä ANALYZING: DATA3\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "   ‚úì Loaded 8,933 orders\n",
      "   ‚úì Loaded 3,466 users\n",
      "   ‚úì Loaded 762 books\n",
      "\n",
      "üßπ Cleaning orders data...\n",
      "   ‚úì Removed 0 invalid rows\n",
      "   ‚úì Final orders count: 8,933\n",
      "\n",
      "üîó Building user deduplication structure...\n",
      "   ‚úì User deduplication complete\n",
      "\n",
      "üìÖ Task 1: Top 5 Revenue Days\n",
      "   1. 2025-02-03: $63,761.34\n",
      "   2. 2024-06-09: $42,015.36\n",
      "   3. 2024-07-26: $38,903.90\n",
      "   4. 2024-11-03: $32,517.20\n",
      "   5. 2024-09-20: $30,660.50\n",
      "\n",
      "üë• Task 2: Real Unique Users\n",
      "   Total records: 3,466\n",
      "   Real unique users: 3,232\n",
      "   Duplicates found: 234\n",
      "\n",
      "‚úçÔ∏è  Task 3: Unique Author Sets\n",
      "   Unique author sets: 268\n",
      "\n",
      "‚≠ê Task 4: Most Popular Author\n",
      "   Most popular: Coy Streich & Keeley Hand & Lela Emard\n",
      "   Books sold: 159\n",
      "\n",
      "üí∞ Task 5: Top Customer\n",
      "   Total spending: $44,582.89\n",
      "   User IDs: [49002, 49414]\n",
      "   Number of aliases: 2\n",
      "\n",
      "üìà Task 6: Daily Revenue Chart\n",
      "   ‚úì Chart saved: daily_revenue_DATA3.png\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DATA3 ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run analysis on all three datasets\n",
    "all_results = {}\n",
    "\n",
    "for dataset in ['DATA1', 'DATA2', 'DATA3']:\n",
    "    try:\n",
    "        all_results[dataset] = analyze_dataset(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error analyzing {dataset}: {str(e)}\\n\")\n",
    "        all_results[dataset] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Part 5: Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "print_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä FINAL RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìÅ DATA1:\n",
      "   Task 1: Top day revenue: $57,011.46\n",
      "   Task 2: Unique users: 3,066\n",
      "   Task 3: Author sets: 325\n",
      "   Task 4: Most popular: Arlinda Huel (201 books)\n",
      "   Task 5: Top customer: [45800] ($37,609.70)\n",
      "   Task 6: Chart: daily_revenue_DATA1.png\n",
      "\n",
      "üìÅ DATA2:\n",
      "   Task 1: Top day revenue: $42,137.01\n",
      "   Task 2: Unique users: 2,633\n",
      "   Task 3: Author sets: 293\n",
      "   Task 4: Most popular: Hershel Treutel & Miss Modesto Denesik & Sen. Trula Bosco (163 books)\n",
      "   Task 5: Top customer: [53256] ($37,051.25)\n",
      "   Task 6: Chart: daily_revenue_DATA2.png\n",
      "\n",
      "üìÅ DATA3:\n",
      "   Task 1: Top day revenue: $63,761.34\n",
      "   Task 2: Unique users: 3,232\n",
      "   Task 3: Author sets: 268\n",
      "   Task 4: Most popular: Coy Streich & Keeley Hand & Lela Emard (159 books)\n",
      "   Task 5: Top customer: [49002, 49414] ($44,582.89)\n",
      "   Task 6: Chart: daily_revenue_DATA3.png\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ALL ANALYSES COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìÅ Generated files:\n",
      "   - daily_revenue_DATA1.png\n",
      "   - daily_revenue_DATA2.png\n",
      "   - daily_revenue_DATA3.png\n",
      "\n",
      "üéØ Next step: Create dashboard with these results\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for dataset in ['DATA1', 'DATA2', 'DATA3']:\n",
    "    results = all_results.get(dataset)\n",
    "    if results is None:\n",
    "        print(f\"{dataset}: Analysis failed\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"üìÅ {dataset}:\")\n",
    "    print(f\"   Task 1: Top day revenue: ${results['task1'].iloc[0]:,.2f}\")\n",
    "    print(f\"   Task 2: Unique users: {results['task2']:,}\")\n",
    "    print(f\"   Task 3: Author sets: {results['task3']:,}\")\n",
    "    \n",
    "    authors = results['task4']['authors']\n",
    "    author_name = authors[0] if len(authors) == 1 else ' & '.join(authors)\n",
    "    print(f\"   Task 4: Most popular: {author_name} ({results['task4']['books_sold']:,} books)\")\n",
    "    \n",
    "    print(f\"   Task 5: Top customer: {results['task5']['user_ids']} (${results['task5']['total_spending']:,.2f})\")\n",
    "    print(f\"   Task 6: Chart: {results['task6']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ ALL ANALYSES COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìÅ Generated files:\")\n",
    "print(\"   - daily_revenue_DATA1.png\")\n",
    "print(\"   - daily_revenue_DATA2.png\")\n",
    "print(\"   - daily_revenue_DATA3.png\")\n",
    "print(\"\\nüéØ Next step: Create dashboard with these results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- All three datasets analyzed independently\n",
    "- Results stored in `all_results` dictionary\n",
    "- Charts saved as PNG files\n",
    "- Ready for dashboard creation\n",
    "\n",
    "**Next Steps:**\n",
    "1. Create dashboard (Streamlit recommended)\n",
    "2. Deploy to Streamlit Cloud or similar\n",
    "3. Submit code repository + dashboard link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
