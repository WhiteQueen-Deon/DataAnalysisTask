{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Part 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "import_libs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions",
   "metadata": {},
   "source": [
    "## Part 2: Utility Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clean_price",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ clean_price function defined\n"
     ]
    }
   ],
   "source": [
    "def clean_price(price_str):\n",
    "    \"\"\"\n",
    "    Clean and standardize price strings to USD floats.\n",
    "    Handles formats: '$27.00', 'â‚¬50Â¢50', 'USD 45.99', 'EUR 71.00'\n",
    "    Converts EUR to USD: â‚¬1 = $1.2\n",
    "    \"\"\"\n",
    "    if pd.isna(price_str):\n",
    "        return None\n",
    "\n",
    "    price_str = str(price_str).strip()\n",
    "    is_euro = ('â‚¬' in price_str) or ('EUR' in price_str.upper())\n",
    "\n",
    "    # Remove currency symbols\n",
    "    price_str = price_str.replace('$', '').replace('â‚¬', '').replace('USD', '')\n",
    "    price_str = price_str.replace('EUR', '').replace('Â¢', '.').strip()\n",
    "    \n",
    "    try:\n",
    "        price = float(price_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # Convert EUR to USD\n",
    "    if is_euro:\n",
    "        price = price * 1.2\n",
    "    \n",
    "    return price\n",
    "\n",
    "print(\"âœ“ clean_price function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "clean_timestamp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ clean_timestamp function defined\n"
     ]
    }
   ],
   "source": [
    "def clean_timestamp(timestamp_str):\n",
    "    \"\"\"\n",
    "    Parse various timestamp formats into pandas datetime.\n",
    "    Handles ISO 8601 (YYYY-MM-DD) and European formats (DD.MM.YYYY)\n",
    "    \"\"\"\n",
    "    if pd.isna(timestamp_str):\n",
    "        return None\n",
    "    \n",
    "    timestamp_str = str(timestamp_str).strip()\n",
    "    timestamp_str = timestamp_str.replace(';', ' ').replace(',', ' ')\n",
    "    timestamp_str = timestamp_str.replace('A.M.', 'AM').replace('P.M.', 'PM')\n",
    "    \n",
    "    # Check if ISO format (YYYY-MM-DD)\n",
    "    iso_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "    if re.search(iso_pattern, timestamp_str):\n",
    "        result = pd.to_datetime(timestamp_str, errors='coerce', dayfirst=False)\n",
    "    else:\n",
    "        result = pd.to_datetime(timestamp_str, errors='coerce', dayfirst=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ“ clean_timestamp function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unionfind",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ UnionFind class defined\n"
     ]
    }
   ],
   "source": [
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union-Find (Disjoint Set Union) data structure with path compression.\n",
    "    Used for efficiently grouping duplicate users.\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    \n",
    "    def find(self, x):\n",
    "        \"\"\"Find root with path compression\"\"\"\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        \"\"\"Merge two sets\"\"\"\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        if root_x != root_y:\n",
    "            self.parent[root_x] = root_y\n",
    "    \n",
    "    def count_groups(self):\n",
    "        \"\"\"Count number of disjoint sets\"\"\"\n",
    "        return len(set(self.find(i) for i in range(len(self.parent))))\n",
    "    \n",
    "    def get_group_members(self, x):\n",
    "        \"\"\"Get all members in the same group as x\"\"\"\n",
    "        root = self.find(x)\n",
    "        return [i for i in range(len(self.parent)) if self.find(i) == root]\n",
    "\n",
    "print(\"âœ“ UnionFind class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_analysis",
   "metadata": {},
   "source": [
    "## Part 3: Main Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "analyze_dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ analyze_dataset function defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Perform complete analysis on a dataset.\n",
    "    Returns dictionary with all 6 task results.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š ANALYZING: {dataset_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # ===== 1. LOAD DATA =====\n",
    "    print(\"ğŸ“‚ Loading data...\")\n",
    "    orders_df = pd.read_parquet(f'{dataset_name}/orders.parquet', engine='fastparquet')\n",
    "    users_df = pd.read_csv(f'{dataset_name}/users.csv')\n",
    "    \n",
    "    with open(f'{dataset_name}/books.yaml', 'r', encoding='utf-8') as file:\n",
    "        books_data = yaml.safe_load(file)\n",
    "    books_df = pd.DataFrame(books_data)\n",
    "    books_df.columns = books_df.columns.str.replace(':', '')\n",
    "    \n",
    "    print(f\"   âœ“ Loaded {len(orders_df):,} orders\")\n",
    "    print(f\"   âœ“ Loaded {len(users_df):,} users\")\n",
    "    print(f\"   âœ“ Loaded {len(books_df):,} books\\n\")\n",
    "    \n",
    "    # ===== 2. CLEAN ORDERS =====\n",
    "    print(\"ğŸ§¹ Cleaning orders data...\")\n",
    "    orders_df['clean_price'] = orders_df['unit_price'].apply(clean_price)\n",
    "    orders_df['clean_timestamp'] = orders_df['timestamp'].apply(clean_timestamp)\n",
    "    \n",
    "    before_clean = len(orders_df)\n",
    "    orders_df = orders_df.dropna(subset=['clean_price', 'clean_timestamp'])\n",
    "    after_clean = len(orders_df)\n",
    "    \n",
    "    orders_df['paid_price'] = orders_df['quantity'] * orders_df['clean_price']\n",
    "    orders_df['date'] = orders_df['clean_timestamp'].dt.date\n",
    "    orders_df = orders_df.drop_duplicates(subset=['id'])\n",
    "    \n",
    "    print(f\"   âœ“ Removed {before_clean - after_clean:,} invalid rows\")\n",
    "    print(f\"   âœ“ Final orders count: {len(orders_df):,}\\n\")\n",
    "    \n",
    "    # ===== 3. BUILD UNION-FIND FOR USERS =====\n",
    "    print(\"ğŸ”— Building user deduplication structure...\")\n",
    "    n_users = len(users_df)\n",
    "    uf = UnionFind(n_users)\n",
    "    users_clean = users_df.copy()\n",
    "    \n",
    "    # Fill missing addresses\n",
    "    na_mask = users_clean['address'].isna()\n",
    "    na_count = na_mask.sum()\n",
    "    users_clean.loc[na_mask, 'address'] = [f'MISSING_ADDRESS_{i}' for i in range(na_count)]\n",
    "    \n",
    "    # Union users with matching fields\n",
    "    for field in ['email', 'phone', 'name', 'address']:\n",
    "        groups = users_clean.groupby(field).groups\n",
    "        for indices in groups.values():\n",
    "            if len(indices) > 1:\n",
    "                first = indices[0]\n",
    "                for idx in indices[1:]:\n",
    "                    uf.union(first, idx)\n",
    "    \n",
    "    print(f\"   âœ“ User deduplication complete\\n\")\n",
    "    \n",
    "    # ===== EXECUTE 6 TASKS =====\n",
    "    results = {}\n",
    "    \n",
    "    # --- TASK 1: Top 5 Revenue Days ---\n",
    "    print(\"ğŸ“… Task 1: Top 5 Revenue Days\")\n",
    "    top5_days = orders_df.groupby('date')['paid_price'].sum().sort_values(ascending=False).head(5)\n",
    "    results['task1'] = top5_days\n",
    "    for i, (date, revenue) in enumerate(top5_days.items(), 1):\n",
    "        print(f\"   {i}. {date}: ${revenue:,.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # --- TASK 2: Unique Users ---\n",
    "    print(\"ğŸ‘¥ Task 2: Real Unique Users\")\n",
    "    unique_users = uf.count_groups()\n",
    "    results['task2'] = unique_users\n",
    "    print(f\"   Total records: {len(users_df):,}\")\n",
    "    print(f\"   Real unique users: {unique_users:,}\")\n",
    "    print(f\"   Duplicates found: {len(users_df) - unique_users:,}\\n\")\n",
    "    \n",
    "    # --- TASK 3: Author Sets ---\n",
    "    print(\"âœï¸  Task 3: Unique Author Sets\")\n",
    "    author_sets = set()\n",
    "    for author_str in books_df['author']:\n",
    "        if pd.isna(author_str):\n",
    "            continue\n",
    "        authors = [a.strip() for a in str(author_str).split(',')]\n",
    "        author_tuple = tuple(sorted(authors))\n",
    "        author_sets.add(author_tuple)\n",
    "    results['task3'] = len(author_sets)\n",
    "    print(f\"   Unique author sets: {len(author_sets):,}\\n\")\n",
    "    \n",
    "    # --- TASK 4: Most Popular Author ---\n",
    "    print(\"â­ Task 4: Most Popular Author\")\n",
    "    book_sales = orders_df.groupby('book_id')['quantity'].sum()\n",
    "    books_with_sales = books_df.merge(book_sales, left_on='id', right_index=True, how='left')\n",
    "    books_with_sales['quantity'] = books_with_sales['quantity'].fillna(0)\n",
    "    \n",
    "    def standardize_author(author_str):\n",
    "        if pd.isna(author_str):\n",
    "            return ()\n",
    "        authors = [a.strip() for a in str(author_str).split(',')]\n",
    "        return tuple(sorted(authors))\n",
    "    \n",
    "    books_with_sales['author_set'] = books_with_sales['author'].apply(standardize_author)\n",
    "    author_sales = books_with_sales.groupby('author_set')['quantity'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    most_popular = author_sales.idxmax()\n",
    "    max_sales = author_sales.max()\n",
    "    results['task4'] = {'authors': most_popular, 'books_sold': int(max_sales)}\n",
    "    \n",
    "    author_name = most_popular[0] if len(most_popular) == 1 else ' & '.join(most_popular)\n",
    "    print(f\"   Most popular: {author_name}\")\n",
    "    print(f\"   Books sold: {int(max_sales):,}\\n\")\n",
    "    \n",
    "    # --- TASK 5: Top Customer ---\n",
    "    print(\"ğŸ’° Task 5: Top Customer\")\n",
    "    user_spending = orders_df.groupby('user_id')['paid_price'].sum()\n",
    "    real_user_spending = {}\n",
    "    \n",
    "    for user_id, spending in user_spending.items():\n",
    "        user_rows = users_clean[users_clean['id'] == user_id]\n",
    "        if len(user_rows) == 0:\n",
    "            continue\n",
    "        user_idx = user_rows.index[0]\n",
    "        root = uf.find(user_idx)\n",
    "        if root not in real_user_spending:\n",
    "            real_user_spending[root] = 0\n",
    "        real_user_spending[root] += spending\n",
    "    \n",
    "    top_real_user = max(real_user_spending, key=real_user_spending.get)\n",
    "    max_spending = real_user_spending[top_real_user]\n",
    "    all_user_indices = uf.get_group_members(top_real_user)\n",
    "    all_user_ids = users_clean.loc[all_user_indices, 'id'].tolist()\n",
    "    \n",
    "    results['task5'] = {'user_ids': all_user_ids, 'total_spending': max_spending}\n",
    "    print(f\"   Total spending: ${max_spending:,.2f}\")\n",
    "    print(f\"   User IDs: {all_user_ids}\")\n",
    "    print(f\"   Number of aliases: {len(all_user_ids)}\\n\")\n",
    "    \n",
    "    # --- TASK 6: Daily Revenue Chart ---\n",
    "    print(\"ğŸ“ˆ Task 6: Daily Revenue Chart\")\n",
    "    daily_revenue = orders_df.groupby('date')['paid_price'].sum().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(daily_revenue.index, daily_revenue.values, \n",
    "             marker='o', linestyle='-', linewidth=2, markersize=4, color='#2E86AB')\n",
    "    plt.fill_between(daily_revenue.index, daily_revenue.values, alpha=0.2, color='#2E86AB')\n",
    "    \n",
    "    plt.title(f'Daily Revenue Over Time - {dataset_name}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Revenue ($)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    chart_filename = f'daily_revenue_{dataset_name}.png'\n",
    "    plt.savefig(chart_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    results['task6'] = chart_filename\n",
    "    print(f\"   âœ“ Chart saved: {chart_filename}\\n\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"âœ… {dataset_name} ANALYSIS COMPLETE\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ“ analyze_dataset function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution",
   "metadata": {},
   "source": [
    "## Part 4: Execute Analysis on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "run_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š ANALYZING: DATA1\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ Loading data...\n",
      "   âœ“ Loaded 11,237 orders\n",
      "   âœ“ Loaded 3,293 users\n",
      "   âœ“ Loaded 753 books\n",
      "\n",
      "ğŸ§¹ Cleaning orders data...\n",
      "   âœ“ Removed 0 invalid rows\n",
      "   âœ“ Final orders count: 11,237\n",
      "\n",
      "ğŸ”— Building user deduplication structure...\n",
      "   âœ“ User deduplication complete\n",
      "\n",
      "ğŸ“… Task 1: Top 5 Revenue Days\n",
      "   1. 2024-12-17: $57,011.46\n",
      "   2. 2024-11-03: $46,258.65\n",
      "   3. 2025-03-23: $39,120.97\n",
      "   4. 2024-09-06: $32,795.31\n",
      "   5. 2025-01-25: $31,732.46\n",
      "\n",
      "ğŸ‘¥ Task 2: Real Unique Users\n",
      "   Total records: 3,293\n",
      "   Real unique users: 3,066\n",
      "   Duplicates found: 227\n",
      "\n",
      "âœï¸  Task 3: Unique Author Sets\n",
      "   Unique author sets: 325\n",
      "\n",
      "â­ Task 4: Most Popular Author\n",
      "   Most popular: Arlinda Huel\n",
      "   Books sold: 201\n",
      "\n",
      "ğŸ’° Task 5: Top Customer\n",
      "   Total spending: $37,609.70\n",
      "   User IDs: [45800]\n",
      "   Number of aliases: 1\n",
      "\n",
      "ğŸ“ˆ Task 6: Daily Revenue Chart\n",
      "   âœ“ Chart saved: daily_revenue_DATA1.png\n",
      "\n",
      "======================================================================\n",
      "âœ… DATA1 ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ANALYZING: DATA2\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ Loading data...\n",
      "   âœ“ Loaded 9,850 orders\n",
      "   âœ“ Loaded 2,810 users\n",
      "   âœ“ Loaded 741 books\n",
      "\n",
      "ğŸ§¹ Cleaning orders data...\n",
      "   âœ“ Removed 0 invalid rows\n",
      "   âœ“ Final orders count: 9,850\n",
      "\n",
      "ğŸ”— Building user deduplication structure...\n",
      "   âœ“ User deduplication complete\n",
      "\n",
      "ğŸ“… Task 1: Top 5 Revenue Days\n",
      "   1. 2024-12-24: $42,137.01\n",
      "   2. 2024-08-29: $40,556.08\n",
      "   3. 2024-12-29: $39,297.21\n",
      "   4. 2025-01-30: $39,021.69\n",
      "   5. 2024-11-29: $35,207.05\n",
      "\n",
      "ğŸ‘¥ Task 2: Real Unique Users\n",
      "   Total records: 2,810\n",
      "   Real unique users: 2,633\n",
      "   Duplicates found: 177\n",
      "\n",
      "âœï¸  Task 3: Unique Author Sets\n",
      "   Unique author sets: 293\n",
      "\n",
      "â­ Task 4: Most Popular Author\n",
      "   Most popular: Hershel Treutel & Miss Modesto Denesik & Sen. Trula Bosco\n",
      "   Books sold: 163\n",
      "\n",
      "ğŸ’° Task 5: Top Customer\n",
      "   Total spending: $37,051.25\n",
      "   User IDs: [53256]\n",
      "   Number of aliases: 1\n",
      "\n",
      "ğŸ“ˆ Task 6: Daily Revenue Chart\n",
      "   âœ“ Chart saved: daily_revenue_DATA2.png\n",
      "\n",
      "======================================================================\n",
      "âœ… DATA2 ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ANALYZING: DATA3\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ Loading data...\n",
      "   âœ“ Loaded 8,933 orders\n",
      "   âœ“ Loaded 3,466 users\n",
      "   âœ“ Loaded 762 books\n",
      "\n",
      "ğŸ§¹ Cleaning orders data...\n",
      "   âœ“ Removed 0 invalid rows\n",
      "   âœ“ Final orders count: 8,933\n",
      "\n",
      "ğŸ”— Building user deduplication structure...\n",
      "   âœ“ User deduplication complete\n",
      "\n",
      "ğŸ“… Task 1: Top 5 Revenue Days\n",
      "   1. 2025-02-03: $63,761.34\n",
      "   2. 2024-06-09: $42,015.36\n",
      "   3. 2024-07-26: $38,903.90\n",
      "   4. 2024-11-03: $32,517.20\n",
      "   5. 2024-09-20: $30,660.50\n",
      "\n",
      "ğŸ‘¥ Task 2: Real Unique Users\n",
      "   Total records: 3,466\n",
      "   Real unique users: 3,232\n",
      "   Duplicates found: 234\n",
      "\n",
      "âœï¸  Task 3: Unique Author Sets\n",
      "   Unique author sets: 268\n",
      "\n",
      "â­ Task 4: Most Popular Author\n",
      "   Most popular: Coy Streich & Keeley Hand & Lela Emard\n",
      "   Books sold: 159\n",
      "\n",
      "ğŸ’° Task 5: Top Customer\n",
      "   Total spending: $44,582.89\n",
      "   User IDs: [49002, 49414]\n",
      "   Number of aliases: 2\n",
      "\n",
      "ğŸ“ˆ Task 6: Daily Revenue Chart\n",
      "   âœ“ Chart saved: daily_revenue_DATA3.png\n",
      "\n",
      "======================================================================\n",
      "âœ… DATA3 ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run analysis on all three datasets\n",
    "all_results = {}\n",
    "\n",
    "for dataset in ['DATA1', 'DATA2', 'DATA3']:\n",
    "    try:\n",
    "        all_results[dataset] = analyze_dataset(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error analyzing {dataset}: {str(e)}\\n\")\n",
    "        all_results[dataset] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Part 5: Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "print_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š FINAL RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ DATA1:\n",
      "   Task 1: Top day revenue: $57,011.46\n",
      "   Task 2: Unique users: 3,066\n",
      "   Task 3: Author sets: 325\n",
      "   Task 4: Most popular: Arlinda Huel (201 books)\n",
      "   Task 5: Top customer: [45800] ($37,609.70)\n",
      "   Task 6: Chart: daily_revenue_DATA1.png\n",
      "\n",
      "ğŸ“ DATA2:\n",
      "   Task 1: Top day revenue: $42,137.01\n",
      "   Task 2: Unique users: 2,633\n",
      "   Task 3: Author sets: 293\n",
      "   Task 4: Most popular: Hershel Treutel & Miss Modesto Denesik & Sen. Trula Bosco (163 books)\n",
      "   Task 5: Top customer: [53256] ($37,051.25)\n",
      "   Task 6: Chart: daily_revenue_DATA2.png\n",
      "\n",
      "ğŸ“ DATA3:\n",
      "   Task 1: Top day revenue: $63,761.34\n",
      "   Task 2: Unique users: 3,232\n",
      "   Task 3: Author sets: 268\n",
      "   Task 4: Most popular: Coy Streich & Keeley Hand & Lela Emard (159 books)\n",
      "   Task 5: Top customer: [49002, 49414] ($44,582.89)\n",
      "   Task 6: Chart: daily_revenue_DATA3.png\n",
      "\n",
      "======================================================================\n",
      "âœ… ALL ANALYSES COMPLETE\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Generated files:\n",
      "   - daily_revenue_DATA1.png\n",
      "   - daily_revenue_DATA2.png\n",
      "   - daily_revenue_DATA3.png\n",
      "\n",
      "ğŸ¯ Next step: Create dashboard with these results\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for dataset in ['DATA1', 'DATA2', 'DATA3']:\n",
    "    results = all_results.get(dataset)\n",
    "    if results is None:\n",
    "        print(f\"{dataset}: Analysis failed\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"ğŸ“ {dataset}:\")\n",
    "    print(f\"   Task 1: Top day revenue: ${results['task1'].iloc[0]:,.2f}\")\n",
    "    print(f\"   Task 2: Unique users: {results['task2']:,}\")\n",
    "    print(f\"   Task 3: Author sets: {results['task3']:,}\")\n",
    "    \n",
    "    authors = results['task4']['authors']\n",
    "    author_name = authors[0] if len(authors) == 1 else ' & '.join(authors)\n",
    "    print(f\"   Task 4: Most popular: {author_name} ({results['task4']['books_sold']:,} books)\")\n",
    "    \n",
    "    print(f\"   Task 5: Top customer: {results['task5']['user_ids']} (${results['task5']['total_spending']:,.2f})\")\n",
    "    print(f\"   Task 6: Chart: {results['task6']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… ALL ANALYSES COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nğŸ“ Generated files:\")\n",
    "print(\"   - daily_revenue_DATA1.png\")\n",
    "print(\"   - daily_revenue_DATA2.png\")\n",
    "print(\"   - daily_revenue_DATA3.png\")\n",
    "print(\"\\nğŸ¯ Next step: Create dashboard with these results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- All three datasets analyzed independently\n",
    "- Results stored in `all_results` dictionary\n",
    "- Charts saved as PNG files\n",
    "- Ready for dashboard creation\n",
    "\n",
    "**Next Steps:**\n",
    "1. Create dashboard (Streamlit recommended)\n",
    "2. Deploy to Streamlit Cloud or similar\n",
    "3. Submit code repository + dashboard link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
